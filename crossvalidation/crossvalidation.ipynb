{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUFdX44RdE88"
   },
   "source": [
    "This notebook repeats all the steps of `train_test_model.ipynb` in a cross-validation framework performed over the train dataset.\n",
    "\n",
    "Before running this notebook it is necessary to split the train dataset TFRecord file into k partitions, follow the instructions in `split_train_set_for_cross_validation.ipynb` in the folder `convert_to_TFRecord`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Y9IEKUevT6"
   },
   "source": [
    "# Import required libraries and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D5NU0ZSJ3Xf"
   },
   "source": [
    "Navigate to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679262940190,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "OE9gS6QyJ2zt",
    "outputId": "090a399f-3eb7-4a1b-ed83-48d09cd9b197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/microsat\n",
      "/home/microsat/hyperspectral-cnn-soil-estimation\n"
     ]
    }
   ],
   "source": [
    "%cd\n",
    "%cd hyperspectral-cnn-soil-estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nktdY6WpeYai"
   },
   "source": [
    "Import libraries and set random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2486,
     "status": "ok",
     "timestamp": 1679262943549,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "Ker0Hmm5PtR9",
    "outputId": "788b35b1-042b-4e5f-8dcb-c282bd140867"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, zipfile, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import random as rn\n",
    "rn.seed(2)\n",
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "SEED = 7231\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "#Disable GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "#Enable full deterministic operations\n",
    "#This options slows down the training process remarkably but it allows to get \n",
    "#deterministic results through different runs of the code\n",
    "#note that results might differ on different hardware (cpu vs gpu) and on different computers\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "from dataset_processing import *\n",
    "from efficientnet_lite import EfficientNetLiteB0mod\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfsHjNylePr7"
   },
   "source": [
    "# List dataset partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679262943724,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "o2h0RGrVPVIe",
    "outputId": "a6b9b6dc-c0c6-4214-d72c-7f0be2f0e078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/train_cv_split_0.record', 'dataset/train_cv_split_1.record', 'dataset/train_cv_split_2.record', 'dataset/train_cv_split_3.record', 'dataset/train_cv_split_4.record']\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'dataset/'\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.startswith(\"train_cv_split\"):\n",
    "        file_list.append(folder_path+file)\n",
    "\n",
    "file_list=sorted(file_list)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUzhdRa5eZhT"
   },
   "source": [
    "# Training and inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNOm4PDkGkPA"
   },
   "source": [
    "Define learning rate scheduling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679262945907,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "IO_HU6EXGjY4"
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, start_lr):\n",
    "    if epoch < 1:\n",
    "      lr = (start_lr-0.001)/1*epoch+0.001\n",
    "    else:\n",
    "      cosine_decay = 0.5 * (1 + tf.cos(np.pi * epoch / (decay_steps)))\n",
    "      decayed = (1 - alpha) * cosine_decay + alpha\n",
    "      lr = start_lr * decayed\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMpR5bSaIgDg"
   },
   "source": [
    "Define a custom metric reflecting the competition scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679262947514,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "Jw2ArlbIIdwQ"
   },
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    y_true = tf.cond(tf.math.equal(label_normalization_mode,0), \n",
    "                     lambda: tf.multiply(y_true, max_labels),\n",
    "                     lambda: tf.multiply(y_true, std_labels)+mean_labels)\n",
    "    y_pred = tf.cond(tf.math.equal(label_normalization_mode,0), \n",
    "                     lambda: tf.multiply(y_pred, max_labels),\n",
    "                     lambda: tf.multiply(y_pred, std_labels)+mean_labels)\n",
    "    \n",
    "    mse = tf.reduce_mean((y_true-y_pred)**2, axis=0)\n",
    "    mse_baseline = [870.02899169921875, 3828.40234375, 1588.857421875, 0.0677162706851959228515625]\n",
    "    score = tf.reduce_mean(mse/mse_baseline)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121262,
     "status": "ok",
     "timestamp": 1679263092785,
     "user": {
      "displayName": "Sistemi Spaziali",
      "userId": "03880471792653564924"
     },
     "user_tz": -60
    },
    "id": "SOc1Fc64L9u-",
    "outputId": "4ef5f83e-64d5-45d7-96fb-ff4184955226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model cv_test0\n",
      "\n",
      "Testing model cv_test0\n",
      "\n",
      "Training model cv_test1\n",
      "\n",
      "Testing model cv_test1\n",
      "\n",
      "Training model cv_test2\n",
      "\n",
      "Testing model cv_test2\n",
      "\n",
      "Training model cv_test3\n",
      "\n",
      "Testing model cv_test3\n",
      "\n",
      "Training model cv_test4\n",
      "\n",
      "Testing model cv_test4\n",
      "\n",
      "Cross validation completed\n",
      "Full log saved in:  submission_files/full_cv_log_cv_test.csv\n"
     ]
    }
   ],
   "source": [
    "noise_std = 0.05\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "target_image_size = 32\n",
    "start_lr = 0.005 \n",
    "networks_base_name = 'cv_test'\n",
    "label_normalization_mode = 0 # 0 for minmax normalization, everything else fot standard normalization\n",
    "\n",
    "###########################################################\n",
    "max_reflectance = 6315\n",
    "test_set_len = 1153\n",
    "max_labels = [325., 625., 400., 14.]\n",
    "std_labels = [29.496254, 61.874084, 39.860474, 0.2602235]\n",
    "mean_labels = [70.30264, 227.98851, 159.28125, 6.782706]\n",
    "\n",
    "#Build and compile the neural network once for all\n",
    "backbone = EfficientNetLiteB0mod(input_shape=(target_image_size, target_image_size, 150),\n",
    "                                   width_coefficient=0.5,\n",
    "                                   depth_coefficient=0.5,\n",
    "                                   dropout_rate=0.1)\n",
    "\n",
    "model = tf.keras.Sequential([backbone,  \n",
    "                            layers.Flatten(),\n",
    "                            layers.Dense(4, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1509))])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(clipnorm=1.),\n",
    "    loss='mse',\n",
    "    metrics=[custom_metric],\n",
    "  )\n",
    "\n",
    "#Define final log path\n",
    "output_csv_log = 'submission_files/full_cv_log_' + networks_base_name + '.csv'\n",
    "\n",
    "i=-1 #Index to iterate over partitions\n",
    "\n",
    "for file in file_list:\n",
    "  #Reset global seeds\n",
    "  rn.seed(2)\n",
    "  np.random.seed(3)\n",
    "  tf.random.set_seed(1)\n",
    "  \n",
    "  #Initialize the model with the same weights we employed during the competition\n",
    "  model.load_weights('efficientnet_lite/initialization_weigths.h5')\n",
    "\n",
    "  #At each iteration remove the i-th file from training file list and use it for testing\n",
    "  i+=1\n",
    "  train_list = file_list.copy()\n",
    "  train_list.remove(file)\n",
    "  test_list = file\n",
    "\n",
    "  #Define train set cardinality\n",
    "  train_set_len = len(list(load_tf_records(train_list)))\n",
    "  num_train_images = (train_set_len//batch_size)*batch_size  \n",
    "\n",
    "  #Define learning rate scheduling callback\n",
    "  alpha = 0.0001\n",
    "  decay_steps = epochs\n",
    "  lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_scheduler(epoch, start_lr), verbose=0)\n",
    "\n",
    "  #Index i denotes the partition used for testing (and excluded from training)\n",
    "  #Therefore cv_test_1 will refer to the model trained on partitions [0,2,..,k] and tested on partition 1\n",
    "  network_name = networks_base_name + str(i) \n",
    "  saved_model_filepath = 'networks/' + network_name \n",
    "  h5_filepath = saved_model_filepath + '/'+network_name+'.h5'\n",
    "  inference_name = 'submission_files/inference_' + network_name + '.csv'\n",
    "\n",
    "  #Dataset preprocessing\n",
    "  steps_per_epoch = num_train_images//batch_size\n",
    "\n",
    "  ds = load_tf_records(train_list).map(decode_dataset_train_val, num_parallel_calls=AUTO)\n",
    "  ds = ds.shuffle(train_set_len, seed=1860)\n",
    "\n",
    "  #Train\n",
    "  train_data = ds.map(lambda patch, label, height, width: normalize_train_val(patch, label, height, width, max_reflectance,  max_labels, mean_labels, std_labels, label_normalization_mode), num_parallel_calls=AUTO,deterministic=True).cache()    #normalize train dataset\n",
    "  train_data = train_data.shuffle(num_train_images, seed=1866)    #shuffle train dataset\n",
    "  train_data = train_data.map(lambda patch, label, height, width: augment_train(patch, label, height, width,target_image_size,noise_std), num_parallel_calls=AUTO,deterministic=True)    \n",
    "  train_data = train_data.batch(batch_size=batch_size, drop_remainder=True)    #batch train dataset\n",
    "  train_data = train_data.prefetch(AUTO)    #prefetch train dataset\n",
    "\n",
    "  #Train the network\n",
    "  print('Training model', network_name)\n",
    "  History = model.fit(train_data,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[lr_callback],\n",
    "                    verbose=0\n",
    "                    )\n",
    "\n",
    "  #Export trained model\n",
    "  model.save(saved_model_filepath)\n",
    "  model.save(h5_filepath)\n",
    "\n",
    "  #Perform inference on holdout set\n",
    "  test_data = load_tf_records(test_list).map(decode_dataset_test, num_parallel_calls=AUTO)\n",
    "\n",
    "  test_data = test_data.map(lambda filename, image, height, width: normalize_test(filename, image, height, width, max_reflectance), num_parallel_calls=AUTO)    #normalize test dataset\n",
    "  test_data = test_data.map(lambda filename, image, height, width: preprocess_test(filename, image, height, width,target_image_size), num_parallel_calls=AUTO).batch(1).prefetch(AUTO)    #batch and prefetch test dataset\n",
    "\n",
    "  filenames = np.array([],dtype=int)\n",
    "    \n",
    "  export_predictions_and_size =  []\n",
    "  print()\n",
    "  print('Testing model', network_name)\n",
    "  print()\n",
    "\n",
    "  for filename, image, height, width in test_data:\n",
    "    filenames = np.append(filenames, int(filename.numpy()[0].decode().replace(\".npz\", \"\")))\n",
    "    predictions = model.predict(image,verbose=0)\n",
    "    \n",
    "    if label_normalization_mode == 0:\n",
    "        \n",
    "        predictions *= max_labels\n",
    "    else:\n",
    "        predictions  = (predictions*std_labels)+mean_labels\n",
    "        \n",
    "    export_predictions_and_size.append(np.concatenate((predictions.reshape([-1]), height.numpy(), width.numpy())))\n",
    "\n",
    "  partition_log = pd.DataFrame(data=export_predictions_and_size, columns=[\"P\", \"K\", \"Mg\", \"pH\",\"height\",\"width\"])\n",
    "  partition_log.index = filenames\n",
    "  partition_log.to_csv(inference_name, index_label=\"sample_index\")\n",
    "\n",
    "#When training is complete merge all csv files to a single one\n",
    "folder_path = 'submission_files/'\n",
    "log_list = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.startswith('inference_' + networks_base_name):\n",
    "        log_list.append(folder_path+file)\n",
    "\n",
    "log_list=sorted(log_list)\n",
    "\n",
    "merged_data = pd.DataFrame()\n",
    "for file in log_list:\n",
    "    data = pd.read_csv(file)\n",
    "    merged_data = pd.concat([merged_data, data])\n",
    "\n",
    "sorted_data=merged_data.sort_values(by=merged_data.columns[0])\n",
    "sorted_data.to_csv(output_csv_log, index=False)\n",
    "print('Cross validation completed')\n",
    "print('Full log saved in: ', output_csv_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4bY7gZ3wXd9RK/ASGtUJp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
